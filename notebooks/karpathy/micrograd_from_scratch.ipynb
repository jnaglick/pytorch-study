{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone of Andrej Karpathy's micrograd, made while following along with his lecture: https://www.youtube.com/watch?v=VMj-3S1tku0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Value Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infixExprLabel(value):\n",
    "    return f\"({value._op.join([str(p.label) for p in value._prev])})\" if value._prev else str(value.data)\n",
    "\n",
    "def prefixExprLabel(value):\n",
    "    return f\"{value._op} {' '.join([str(p.label) for p in value._prev])} \" if value._prev else str(value.data)\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, data, _prev=(), _op='', label=None):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        self._prev = _prev\n",
    "        self._op = _op\n",
    "        self.label = label if label else infixExprLabel(self)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{type(self).__name__}(data={self.data:5f} grad={self.grad:5f})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        return AddOp(self, other)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        return MulOp(self, other)\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        return PowerOp(self, other)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + -1 * other\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return self * (other**-1)\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "    \n",
    "    def __rsub__(self, other): # other - self\n",
    "        return self - other\n",
    "    \n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "    \n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return self / other\n",
    "\n",
    "    def __neg__(self):\n",
    "        return -1 * self\n",
    "    \n",
    "    def tanh(self):\n",
    "        return TanhOp(self)\n",
    "    \n",
    "    def _backwards(self):\n",
    "        pass\n",
    "\n",
    "class BinaryOp(Value):\n",
    "    def __init__(self, data, a, b, _op):\n",
    "        super().__init__(data, (a, b), _op)\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "class PowerOp(BinaryOp):\n",
    "    def __init__(self, a, b):\n",
    "        super().__init__(a.data ** b.data, a, b, '**')\n",
    "\n",
    "    def _backwards(self):\n",
    "        self.a.grad += self.b.data * self.a.data**(self.b.data-1) * self.grad\n",
    "        # self.b.grad += (math.log(self.a.data) * self.a.data**self.b.data) * self.grad\n",
    "\n",
    "class AddOp(BinaryOp):\n",
    "    def __init__(self, a, b):\n",
    "        super().__init__(a.data + b.data, a, b, '+')\n",
    "\n",
    "    def _backwards(self):\n",
    "        self.a.grad += self.grad\n",
    "        self.b.grad += self.grad\n",
    "\n",
    "class MulOp(BinaryOp):\n",
    "    def __init__(self, a, b):\n",
    "        super().__init__(a.data * b.data, a, b, '*')\n",
    "\n",
    "    def _backwards(self):\n",
    "        self.a.grad += self.b.data * self.grad\n",
    "        self.b.grad += self.a.data * self.grad\n",
    "\n",
    "class TanhOp(Value):\n",
    "    def __init__(self, a):\n",
    "        tanh = (math.exp(2*a.data) - 1) / (math.exp(2*a.data) + 1)\n",
    "        super().__init__(tanh, (a,), 'tanh', f\"tanh({a.label})\")\n",
    "        self.a = a\n",
    "\n",
    "    def _backwards(self):\n",
    "        self.a.grad += (1 - self.data**2) * self.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Neural Net**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(x, y):\n",
    "    assert len(x) == len(y), \"shape mismatch x: {}, y: {}\".format(len(x), len(y))\n",
    "    sum = x[0] * y[0]\n",
    "    for i in range(1, len(x)):\n",
    "        sum += x[i] * y[i]\n",
    "    return sum\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, in_features):\n",
    "        self.weights = [Value(w, label='w') for w in np.random.randn(in_features)]\n",
    "        self.bias = Value(np.random.randn(), label='b')\n",
    "        self.activation = Value(0.)\n",
    "\n",
    "    def activate(self, x):\n",
    "        self.activation = (dot(self.weights, x) + self.bias).tanh()\n",
    "        self.activation.label = 'a'\n",
    "        return self.activation\n",
    "\n",
    "class FullyConnectedLayer:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.neurons = [Neuron(in_features) for _ in range(out_features)]\n",
    "\n",
    "    def activate(self, x):\n",
    "        return [neuron.activate(x) for neuron in self.neurons]\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.layers = [\n",
    "            FullyConnectedLayer(input_size, hidden_size), \n",
    "            FullyConnectedLayer(hidden_size, output_size)\n",
    "        ]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.activate(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**draw_graph()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph(node, connectToNodeId=None, graph=None):\n",
    "    if graph is None:\n",
    "        graph = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n",
    "\n",
    "    nodeId = str(id(node))\n",
    "\n",
    "    # draw me:\n",
    "    label = f\"{node.label} | {node.data:.5f} | grad: {node.grad:0.5f}\" if node.label else f\"{node.data:.5f} | grad: {node.grad:0.5f}\"\n",
    "    graph.node(nodeId, label = label, shape='record')\n",
    "\n",
    "    if connectToNodeId:\n",
    "        graph.edge(nodeId, connectToNodeId)\n",
    "\n",
    "    # if result of operator, draw operator node:\n",
    "    if node._op:\n",
    "        opNodeId = str(id(node)) + node._op\n",
    "        graph.node(opNodeId, label=node._op, shape='circle') # draw operator node\n",
    "        graph.edge(opNodeId, nodeId) # connect me to it\n",
    "\n",
    "        for p in node._prev:\n",
    "            draw_graph(p, opNodeId, graph)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drawing the numeric derivative**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nh = Value(0.001, label='h')\\n\\n# L = -2 * (2.0 * -3.0 + 10.0)\\na = Value(2.0, label='a')\\nb = Value(-3.0, label='b')\\nc = Value(10.0, label='c')\\ne = a * b; e.label = 'e'\\nd = e + c; d.label = 'd'\\nf = Value(-2.0, label='f')\\nL = d * f; L.label = 'L1'\\nL1 = L\\n\\n#     F     A     B      C\\n# L = -2 * (2.0 * -3.0 + 10.0)\\na = Value(2.0, label='a')\\nb = Value(-3.0, label='b')\\nc = Value(10.0, label='c')\\ne = a * b; e.label = 'e'\\ne = e + h; e.label = 'derr_e'\\nd = e + c; d.label = 'd'\\nf = Value(-2.0, label='f')\\nL = d * f; L.label = 'L2'\\nL2 = L\\n\\nx = (L2 - L1)/h; x.label = 'numeric derivative'\\n# draw_graph(x)\\n\""
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "h = Value(0.001, label='h')\n",
    "\n",
    "# L = -2 * (2.0 * -3.0 + 10.0)\n",
    "a = Value(2.0, label='a')\n",
    "b = Value(-3.0, label='b')\n",
    "c = Value(10.0, label='c')\n",
    "e = a * b; e.label = 'e'\n",
    "d = e + c; d.label = 'd'\n",
    "f = Value(-2.0, label='f')\n",
    "L = d * f; L.label = 'L1'\n",
    "L1 = L\n",
    "\n",
    "#     F     A     B      C\n",
    "# L = -2 * (2.0 * -3.0 + 10.0)\n",
    "a = Value(2.0, label='a')\n",
    "b = Value(-3.0, label='b')\n",
    "c = Value(10.0, label='c')\n",
    "e = a * b; e.label = 'e'\n",
    "e = e + h; e.label = 'derr_e'\n",
    "d = e + c; d.label = 'd'\n",
    "f = Value(-2.0, label='f')\n",
    "L = d * f; L.label = 'L2'\n",
    "L2 = L\n",
    "\n",
    "x = (L2 - L1)/h; x.label = 'numeric derivative'\n",
    "# draw_graph(x)\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backprop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topological_sort(node, ordered=[]):\n",
    "    if node not in ordered:\n",
    "        for p in node._prev:\n",
    "            topological_sort(p, ordered)\n",
    "        ordered.append(node)\n",
    "    return ordered\n",
    "\n",
    "def backprop(node):\n",
    "    node.grad = 1\n",
    "    for n in reversed(topological_sort(node)):\n",
    "        n._backwards()\n",
    "\n",
    "def zero_grad(node):\n",
    "    for n in topological_sort(node):\n",
    "        n.grad = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, y_hat):\n",
    "    return (y_hat - y)**2\n",
    "\n",
    "def average_loss(net, loss_function, xs, ys):\n",
    "    loss = 0\n",
    "    for i in range(len(xs)):\n",
    "        output = net.forward(xs[i])\n",
    "        loss += loss_function(ys[i], output[0])\n",
    "    return loss / len(xs)\n",
    "\n",
    "def optimize(net, learning_rate):\n",
    "    for layer in net.layers:\n",
    "        for neuron in layer.neurons:\n",
    "            for weight in neuron.weights:\n",
    "                weight.data -= weight.grad * learning_rate\n",
    "            neuron.bias.data -= neuron.bias.grad * learning_rate\n",
    "\n",
    "def train(net, xs, ys, epochs=1000, learning_rate=0.01):\n",
    "    for epoch in range(epochs):\n",
    "        loss = average_loss(net, MSE, xs, ys)\n",
    "        zero_grad(loss)\n",
    "        backprop(loss)\n",
    "        optimize(net, learning_rate)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected:\n",
      "[1, 1, 0, 0, 1, 1, 0, 0]\n",
      "Initial predictions (loss=0.6611033788456152)\n",
      "[0.9178460415031388, 0.9313390598221278, 0.8730611747173703, 0.7333214725895104, 0.999884098451666, -0.6163202097959993, 0.9997098409151506, 0.6045299299055742]\n",
      "Final predictions (loss=0.2605164885673867)\n",
      "[0.8674074814756224, 0.808605786746208, 0.985846258912072, 0.13999306039473336, 0.9999666655622258, 0.8108573687247611, 0.9998528835718159, -0.05429327462978548]\n"
     ]
    }
   ],
   "source": [
    "xs = np.random.randn(8, 4)\n",
    "ys = [1, 1, 0, 0, 1, 1, 0, 0]\n",
    "net = Network(input_size=4, hidden_size=16, output_size=1)\n",
    "\n",
    "print(\"expected:\")\n",
    "print(ys)\n",
    "\n",
    "loss = average_loss(net, MSE, xs, ys)\n",
    "print(f\"Initial predictions (loss={loss.data})\")\n",
    "\n",
    "predictions = [net.forward(x)[0].data for x in xs]\n",
    "print(predictions)\n",
    "\n",
    "# one training step:\n",
    "# zero_grad(loss)\n",
    "# backprop(loss)\n",
    "# optimize(net, 0.01)\n",
    "\n",
    "# training loop:\n",
    "train(net, xs, ys, epochs=30, learning_rate=0.05)\n",
    "\n",
    "loss = average_loss(net, MSE, xs, ys)\n",
    "print(f\"Final predictions (loss={loss.data})\")\n",
    "\n",
    "predictions = [net.forward(x)[0].data for x in xs]\n",
    "print(predictions)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cool graph:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_graph(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
